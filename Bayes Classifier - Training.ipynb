{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA_FILE = \"02_Training/train-data.txt\"\n",
    "TEST_DATA_FILE = \"02_Training/test-data.txt\"\n",
    "\n",
    "TOKEN_SPAM_PROB_FILE = \"03_Testing/prob-spam.txt\"\n",
    "TOKEN_HAM_PROB_FILE = \"03_Testing/prob-ham.txt\"\n",
    "TOKEN_ALL_PROB_FILE = \"03_Testing/prob-all.txt\"\n",
    "\n",
    "TEST_FEATURE_MATRIX = \"03_Testing/test-feature.txt\"\n",
    "TEST_TARGET_FILE = \"03_Testing/test-target.txt\"\n",
    "\n",
    "VOCAB_SIZE = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_train_data = np.loadtxt(TRAINING_DATA_FILE, delimiter=' ', dtype=int)\n",
    "sparse_test_data = np.loadtxt(TEST_DATA_FILE, delimiter=' ', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1724"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(sparse_test_data[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_full_matrix(sparse_matrix, nr_words, doc_idx=0, word_idx=1, cat_idx=2, freq_idx=3):\n",
    "    column_names = [\"DOC_ID\"] + [\"CATEGORY\"] + list(range(VOCAB_SIZE))\n",
    "    doc_id_names = np.unique(sparse_matrix[:, 0])\n",
    "    full_matrix = pd.DataFrame(index=doc_id_names, columns=column_names)\n",
    "    full_matrix.fillna(0, inplace=True)\n",
    "    for i in range(sparse_matrix.shape[0]):\n",
    "        doc_nr = sparse_matrix[i][doc_idx]\n",
    "        word_id = sparse_matrix[i][word_idx]\n",
    "        label = sparse_matrix[i][cat_idx]\n",
    "        occurrence = sparse_matrix[i][freq_idx]\n",
    "        \n",
    "        full_matrix.at[doc_nr, \"DOC_ID\"] = doc_nr\n",
    "        full_matrix.at[doc_nr, \"CATEGORY\"] = label\n",
    "        full_matrix.at[doc_nr, word_id] = occurrence\n",
    "    full_matrix.set_index(\"DOC_ID\", inplace=True)\n",
    "    return full_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_data = make_full_matrix(sparse_train_data, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Total # of Tokens (Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_features = full_train_data.loc[:, full_train_data.columns!=\"CATEGORY\"]\n",
    "total_words = full_train_features.sum(axis=1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find # of Tokens in Spam & Ham Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178663 252325\n"
     ]
    }
   ],
   "source": [
    "spam_train_features = full_train_data.loc[full_train_data[\"CATEGORY\"]!=0, full_train_data.columns!=\"CATEGORY\"]\n",
    "total_spam_words = spam_train_features.sum(axis=1).sum()\n",
    "ham_train_features = full_train_data.loc[full_train_data[\"CATEGORY\"]!=1, full_train_data.columns!=\"CATEGORY\"]\n",
    "total_ham_words = ham_train_features.sum(axis=1).sum()\n",
    "print(total_spam_words, total_ham_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find # of Certain Kinds of Tokens in Spam & Ham Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       2177\n",
      "1        936\n",
      "2       1220\n",
      "3       2026\n",
      "4       1211\n",
      "        ... \n",
      "2495      23\n",
      "2496       2\n",
      "2497       4\n",
      "2498       2\n",
      "2499      25\n",
      "Length: 2500, dtype: int64 0       5483\n",
      "1       2588\n",
      "2       2044\n",
      "3        938\n",
      "4       1611\n",
      "        ... \n",
      "2495       1\n",
      "2496      22\n",
      "2497      30\n",
      "2498      37\n",
      "2499       3\n",
      "Length: 2500, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "summed_spam_tokens = (full_train_features.loc[full_train_data[\"CATEGORY\"]!=0]).sum(axis=0)+1\n",
    "summed_ham_tokens = (full_train_features.loc[full_train_data[\"CATEGORY\"]!=1]).sum(axis=0)+1\n",
    "print(summed_spam_tokens, summed_ham_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding P(Token | Spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.012017\n",
      "1       0.005167\n",
      "2       0.006734\n",
      "3       0.011183\n",
      "4       0.006685\n",
      "          ...   \n",
      "2495    0.000127\n",
      "2496    0.000011\n",
      "2497    0.000022\n",
      "2498    0.000011\n",
      "2499    0.000138\n",
      "Length: 2500, dtype: float64 0       0.021517\n",
      "1       0.010156\n",
      "2       0.008021\n",
      "3       0.003681\n",
      "4       0.006322\n",
      "          ...   \n",
      "2495    0.000004\n",
      "2496    0.000086\n",
      "2497    0.000118\n",
      "2498    0.000145\n",
      "2499    0.000012\n",
      "Length: 2500, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "prob_tokens_spam = summed_spam_tokens/(total_spam_words+2500)\n",
    "prob_tokens_ham = summed_ham_tokens/(total_ham_words+2500)\n",
    "print(prob_tokens_spam, prob_tokens_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding P(Token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_tokens_all = full_train_features.sum(axis=0)/total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(TOKEN_SPAM_PROB_FILE, prob_tokens_spam)\n",
    "np.savetxt(TOKEN_HAM_PROB_FILE, prob_tokens_ham)\n",
    "np.savetxt(TOKEN_ALL_PROB_FILE, prob_tokens_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_data = make_full_matrix(sparse_test_data, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_features = full_test_data.loc[:, full_test_data.columns!=\"CATEGORY\"]\n",
    "full_test_target = full_test_data[\"CATEGORY\"]\n",
    "np.savetxt(TEST_FEATURE_MATRIX, full_test_features)\n",
    "np.savetxt(TEST_TARGET_FILE, full_test_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
